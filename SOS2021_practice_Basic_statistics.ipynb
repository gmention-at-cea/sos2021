{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOS2021_practice_Basic_statistics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVhmLG3wruOe"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We focus here on pure simulations. We target Monte Carlo simulations in this notebook to understand basic statistics and give you ways to improve yourself after SOS 2021...\n",
        "\n",
        "In this session we plan to cover the following topics:\n",
        "* Accessing common probability distributions\n",
        "* Drawing random samples (predefined or custom)\n",
        "* Plotting distributions (scatter plots, histograms, errorbars)\n",
        "* Fitting (chi square $\\chi^2$ and Maximum likelihood estimators)\n",
        "* Understanding covariance (correlation, partial correlation)\n",
        "\n",
        "We won't have time to cover confidence intervals (p-value, significance, testing) in this session.\n",
        "\n",
        "We will thus be in line with the first series of lectures provided by J. Donini on Monday.\n",
        "\n",
        "In this practice session we target multi-levels attendees: we cover basic ideas of statistics but also provide higher level features for already experienced participants so that every one can come back with something in the backpack...\n",
        "\n",
        "# The practice environement - Colaboratory\n",
        "**Colaboratory** = python notebooks (Jupyter) on Google servers. Convenient to share, teach, provide demonstrations... Only suited for small projects or small code. Otherwise use development environment on your computer (Atom, Pycharm, Spyder, Sublime Text,...). You can still develop large python classes and scripts (`.py` files) upload them on Google Drive and call them in your notebooks...\n",
        "\n",
        "Main **purpose** of Colaboratory is thus: **work together and share**.\n",
        "Advantages:\n",
        "* Write and execute python code in your web browser (even on smartphones, tablets,...)\n",
        "* Create/upload and share notebooks\n",
        "* Import and save notebooks from/to Google Drive or GitHub\n",
        "* Plus a lot of extras: import export datas (pandas, tensorflow, keras, use provided GPU or TPU by Google) etc..\n",
        "\n",
        "**A python notebook** is a file (usually \".ipynb\") with both `TEXT` and `CODE`, splitted in separate cells which forms the notebook. `TEXT` cells help describe the frame and the computations for demonstration purposes. `CODE` cells are executable and can share variables in the notebook workspace environement (kernel) and produce outputs (printing, tables, graphics). That's all to start with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjOJpsbgPzqZ"
      },
      "source": [
        "# Basic packages to work with\n",
        "\n",
        "In this notebook we will use widespread packages:\n",
        "- For numerical computations: `numpy`\n",
        "- From `scipy` ([SciPy documentation](https://docs.scipy.org/doc/scipy/reference/tutorial/general.html)):\n",
        "  - For probabilities: `scipy.stats`\n",
        "  - Optimizations (finding minimum...): `scipy.optimize`\n",
        "- For symbolic computations: `sympy`\n",
        "- For plotting: `matplotlib`\n",
        "\n",
        "That's all, to start with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kHRQadRBQK4"
      },
      "source": [
        "We start importing packages and with a few cosmetic definitions for layout, graphs, display.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmLwSvvcKLdz"
      },
      "source": [
        "try:\n",
        "    # just in case for better display on a Retina screen on MacOSX\n",
        "    # %config InlineBackend.figure_format = {'retina'} # 'svg'\n",
        "    from IPython.display import set_matplotlib_formats  \n",
        "    set_matplotlib_formats('pdf', 'svg')\n",
        "    %config InlineBackend.figure_format ='svg'\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import sympy as sym\n",
        "from IPython.display import Math\n",
        "# Latex printing of equations - useful with symbolic package sympy for instance.\n",
        "def laprint(x):\n",
        "  return display(Math(sp.latex(x)))\n",
        "\n",
        "import matplotlib as mpl # [m]athematics [p]lotting [l]ibrary\n",
        "# mpl.rcParams[\"mathtext.default\"] ='rm'\n",
        "mpl.rcParams['mathtext.default'] = 'regular'\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('ggplot')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiKuMypsMlF"
      },
      "source": [
        "**A word about colors and style in plots**\n",
        "\n",
        "Here is a nice default color palette to use in your graphics with `matplotlib`.\n",
        "\n",
        "To use one of these colors use for instance `color='tab:red'` in your graphic call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI1nhfFDsLpE"
      },
      "source": [
        "import matplotlib.colors as colors\n",
        "colors.TABLEAU_COLORS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fPSnu39sDwy"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/gmention-at-cea/ST4_PP_2020/master/tableau_named_colors.png\" width=\"800px\">\n",
        "<figcaption><b>Fig. 1</b> | Name of some nice basic colors in matplotlib using the Tableau palette.</figcaption>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Mi3xs4p8fv"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tU5q166oqph"
      },
      "source": [
        "# Statistical inference\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mcZAGx71ngbeJ4wXZNbI97XdS4DtvTW5\" width = 800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgj-1RxKQ1mF"
      },
      "source": [
        "# Probabilities\n",
        "\n",
        "* During the statistics lectures of J. Donini, you have seen a couple of **distributions**.\n",
        "\n",
        "* **Probabilities** are used to model the **random errors** from the measurements in Physics experiment.  \n",
        "Two basic nature of errors: **continuous** and **discrete**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haaIOyBvcwQh"
      },
      "source": [
        "Distribution probabilities in `scipy.stats`. \n",
        "For each probability distribution, you can access  to *METHODS*:\n",
        "> * Random variates: `rvs(loc=0, scale=1, size=1, random_state=None)`  \n",
        "> * Probability density function (PDF): `pdf(x, loc=0, scale=1)`  \n",
        "> * Cumulative distribution function (CDF): `cdf(x, loc=0, scale=1)`  \n",
        "> * Quantile function (functional inverse of the CDF. Percent point function): ` ppf(q, loc=0, scale=1)`  \n",
        "> * Median, mean, variance, standard deviation: `median`, `mean`, `var`, `std`  \n",
        "\n",
        "To get help, you can write this in the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6tP1fJMLNpE"
      },
      "source": [
        "stats.norm?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilttgQRXfydw"
      },
      "source": [
        "\n",
        "and hit '**Shift**'+'**Return**' after the `?` on your **keyboard** or execute the cell with the **play button icon** on the left of the cell code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2uchgxPf_y4"
      },
      "source": [
        "## Tabulated distributions - howtos\n",
        "\n",
        "*Objectives:*\n",
        "* Plotting probability density or mass functions of standard distributions\n",
        "* Drawing random variables from a given distribution\n",
        "\n",
        "First, you want to know the available distributions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le86ZSCXgIf5"
      },
      "source": [
        "# Hit TAB key in the following line after the dot of stats. You should see a\n",
        "# popup scrolling window \n",
        "stats."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oYeRhhIh4VQ"
      },
      "source": [
        "Alternatively, you can use the `scipy.stats` [documentation](https://docs.scipy.org/doc/scipy/reference/stats.html) to access the descriptions with a better display formatting... Up to you..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrAn4QlicrIt"
      },
      "source": [
        "\n",
        "### Plotting some probability distributions\n",
        "**Uniform distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEZ-izOBcsNI"
      },
      "source": [
        "# Define a uniform probability distribution between 'a' and 'b'\n",
        "a = 2\n",
        "b = 5\n",
        "U = stats.uniform(loc=a, scale=b)\n",
        "# We define a range of values to evaluate the PDF, CDF\n",
        "x = np.linspace(0,10,1000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,3))\n",
        "ax1.plot(x, U.pdf(x))\n",
        "ax1.set_xlabel('$x$')\n",
        "ax1.set_ylabel('PDF: $f_U(x)$')\n",
        "ax1.set_title('Density distribution function')\n",
        "\n",
        "ax2.plot(x, U.cdf(x))\n",
        "ax2.set_xlabel('$x$')\n",
        "ax2.set_ylabel('CDF: $F_U(x)$')\n",
        "ax2.set_title('Cumulative distribution function')\n",
        "# p = np.linspace(0,1,1000)\n",
        "# plt.plot(p, U.ppf(p))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouIN6JHkJzpX"
      },
      "source": [
        "**EXERCISE**: Do the same plots for a normal distribution with mean 5 and standard deviation 1.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzGkU1X3KBuT"
      },
      "source": [
        "# X variable should contain the normal distribution from stats package.\n",
        "\n",
        "mu = 5\n",
        "sigma = 1.2\n",
        "X = stats.norm(loc=mu, scale=sigma)\n",
        "\n",
        "\n",
        "x = np.linspace(0,10,1000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,3))\n",
        "ax1.plot(x, X.pdf(x))\n",
        "ax1.set_xlabel('$x$')\n",
        "ax1.set_ylabel('PDF: $f_U(x)$')\n",
        "ax1.set_title('Density distribution function')\n",
        "\n",
        "ax2.plot(x, X.cdf(x))\n",
        "ax2.set_xlabel('$x$')\n",
        "ax2.set_ylabel('CDF: $F_U(x)$')\n",
        "ax2.set_title('Cumulative distribution function')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMj3hIUiXnmo"
      },
      "source": [
        "z0 = -0.7\n",
        "\n",
        "# To have fun with sliders, uncomment the line below\n",
        "# z0 = -0.9 #@param {type:\"slider\", min:-3, max:3, step:0.1}\n",
        "\n",
        "# or you can use the \"Insert > Add a form field\" menu of Google Colab.\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,3))\n",
        "x = np.arange(-3,3,0.001)\n",
        "mu = 0\n",
        "sigma =1\n",
        "\n",
        "z = x[x<z0]\n",
        "f0 = stats.norm.pdf(z0, mu, sigma)\n",
        "ax1.plot(x, stats.norm.pdf(x, mu, sigma))\n",
        "ax1.fill_between(z, 0, stats.norm.pdf(z, mu, sigma), alpha=0.25, color='k')\n",
        "ax1.plot(z0, f0, 'ko')\n",
        "ax1.plot([z0, z0],[0, stats.norm.pdf(z0, mu, sigma)], 'k--')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('$f_X(x)$')\n",
        "ax1.set_title('Density')\n",
        "\n",
        "F0 = stats.norm.cdf(z0, mu, sigma)\n",
        "ax2.plot(x, stats.norm.cdf(x, mu, sigma))\n",
        "ax2.plot(z0, F0, 'ko')\n",
        "ax2.plot([z0, z0],[0, F0], 'k--')\n",
        "ax2.plot([-3, z0],[F0, F0], 'k:')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('$F_X(x)$')\n",
        "ax2.set_title('Cumulative')\n",
        "\n",
        "p = np.linspace(0,1,1000)\n",
        "ax3.plot(p, stats.norm.ppf(p, mu, sigma))\n",
        "ax3.plot(F0, z0, 'ko')\n",
        "ax3.plot([0, F0], [z0, z0], 'k--')\n",
        "ax3.plot([F0, F0], [-3, z0], 'k:')\n",
        "ax3.set_xlabel('p')\n",
        "ax3.set_ylabel('$Q_X(p)=F_X^{-1}(p)$')\n",
        "ax3.set_title('Quantile')\n",
        "    \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0fEsTwPgUw"
      },
      "source": [
        "**EXERCISE**: Using the binomial distribution, plot a histogram of a sample of the number of successes in 100 experiments of 10 trials with 80% of sucess each.\n",
        "\n",
        "Notice: for a better reading of the plot, be careful with the bin centering, bin edges, \n",
        "bin width..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlzixneLIM2V"
      },
      "source": [
        "\n",
        "n_trial = 10\n",
        "success_proba = 0.8\n",
        "B = stats.binom(n_trial, success_proba)\n",
        "n_experiments = 100\n",
        "\n",
        "Brvs = B.rvs(n_experiments)\n",
        "\n",
        "plt.hist(Brvs,bins=range(0,12), edgecolor='k', align='left', rwidth=.5)\n",
        "plt.xlabel('Number of successes in 10 trials')\n",
        "plt.ylabel('Success counts with %d experiments' % n_experiments)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkBC2gnR4pX"
      },
      "source": [
        "**EXERCISE**: Give the mean, variance, skewness and kurtosis of a binomial with n=20 trials and success probability p=0.4. You just need 1 line of code for all this. How?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r739NX9VSF03"
      },
      "source": [
        "\n",
        "\n",
        "stats.binom.stats(20,.4, moments='mvsk')\n",
        "\n",
        "\n",
        "\n",
        "# Computing summary statistics one by one\n",
        "print('Bino mean = %.2f, var= %.2f, std = %.2f' % (B.mean(), B.var(), B.std()))\n",
        "# Computing several summary statistics at once\n",
        "Bmean, Bvar, Bskew, Bkurt = B.stats('mvsk')\n",
        "print('Bino mean = %.2f, var= %.2f, skew =%.2f, kurt = %.2f' % \\\n",
        "      (Bmean, Bvar, Bskew, Bkurt))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScWkffgTqit"
      },
      "source": [
        "## Custom distribution - howtos\n",
        "\n",
        "You would like to define your own PDF?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP9-nDT-2Qy5"
      },
      "source": [
        "# A completely arbitrary (but positive) function\n",
        "def my_custom_pdf(x, p):\n",
        "  # defined on [0; 4] interval with f: x -> x*(p0-x)*sin(p1*x)\n",
        "  # It has 2 parameters p0 and p1 given in array p.\n",
        "  return np.array([ np.piecewise(x, \\\n",
        "                        [x < 0, x >= 4, x>=0 and x<4                     ],\\\n",
        "                        [0    , 0     , np.abs(x*(p[0]-x)*np.sin(p[1]*x))])\\\n",
        "           for x in x])\n",
        "\n",
        "# Let's check its behaviour on some values\n",
        "x = np.linspace(-2,6,1000)\n",
        "plt.plot(x, my_custom_pdf(x, np.array((1,1))))\n",
        "plt.plot(x, my_custom_pdf(x, np.array((-1,np.pi))))\n",
        "plt.title('Not normalized functions for PDFs', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwhkUrIO7LxD"
      },
      "source": [
        "# Let's generate random samples from them. How?\n",
        "from scipy.interpolate import interp1d \n",
        "x = np.linspace(0, 4, 1000)\n",
        "\n",
        "# Quick and dirty quantile definition\n",
        "par1 = (1, 1)\n",
        "fx = my_custom_pdf(x, par1) # not normalized...\n",
        "Fx = np.cumsum(fx)/np.sum(fx) # Computing the CDF\n",
        "Qx = interp1d(Fx, x, kind='linear')\n",
        "p = stats.uniform.rvs(size=10000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,2))\n",
        "ax1.hist(Qx(p), bins=50, density=True)\n",
        "ax1.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax1.set_title(f'Distribution for p=(%.0f, %.0f)' % par1, fontsize=12)\n",
        "\n",
        "par2 = (-1,np.pi)\n",
        "fx = my_custom_pdf(x, par2)\n",
        "Fx = np.cumsum(fx)/np.sum(fx)\n",
        "Qx = interp1d(Fx, x, kind='linear')\n",
        "\n",
        "ax2.hist(Qx(p), bins=50, density=True)\n",
        "ax2.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax2.set_title(f'Distribution for p=(%.0f, %.2f)' % par2, fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByZ7W4Xyzt0Z"
      },
      "source": [
        "You can even imagine to get a extremely regular sample. Instead of drawing random values for $p$ from the uniform distribution in $[0;1]$ you can use a equally spaced array for $p$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm2XZaOzzCMl"
      },
      "source": [
        "nsample = 200\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,2))\n",
        "p = np.linspace(0+1e-4, 1-1e-4, nsample)\n",
        "ax1.hist(Qx(p), bins=20, density=True)\n",
        "ax1.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax1.set_title('Equally spaced p', fontsize=12)\n",
        "\n",
        "np.random.seed(seed=123456)\n",
        "p = stats.uniform.rvs(size=nsample)\n",
        "ax2.hist(Qx(p), bins=20, density=True)\n",
        "ax2.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax2.set_title('p from random uniform', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KanMa8f10Kao"
      },
      "source": [
        "This kind of trick can be used to check calculations with random variables with lower size samples without fluctuations for \"debugging purposes\"..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fLM6ZrXT_F2"
      },
      "source": [
        "# Histograms\n",
        "\n",
        "One of the simplest graphical view of PDF (continuous random variable), PMF (discrete). Data collection into classes, called **bins**.  \n",
        "Either discrete data: simple choice of classes given by default by the discrete characteristics of the random variable. Or for continuous random variable: decide for the range of values. It's an arbitrary decision. Based on some criteria. Arbitrary number of bins. Arbitrary bin sizes. All bins with the same width? Not? Where to start and end? \n",
        "\n",
        "Answer: Up to you... + some clever modelling sometime,...  \n",
        "or simply experimental expectation to drive the choice.  \n",
        "\n",
        "Usually the case for physicists: be pragmatic. Do not enter in complex mathematical theory of expectation, information theory and so on. Just find a binning choice sufficient to sum up data in less categories than initially available. This usually simplify the analysis in 2 ways:\n",
        "1. lower the dimensionality of the variations to the number of bins and not the number of data points and gives often faster computations;\n",
        "2. \"blur\" the raw data in some interval... Make us unsensitive to peculiar values inside the range. This sets the limit to our precision we can reach in the model and its parameters.\n",
        "\n",
        "Why not use raw data? Why kind of blur raw data into bins?  \n",
        "\n",
        "Answer: To put a safety belt on what could happen when you open pandora box of your data sample... Remember that all the frequentist statistics business has to be set up prior to seeing data (see J. Donini and G. Cowan lectures). This is for all the coverage of confidence intervals and so on. \n",
        "\n",
        "**Every model is wrong**: \n",
        "If you collect data long enough, chances are your model will no longer be able to describe the data. Discordent effects may occur because of the accuracy of the data collected. These lack of model accuracy effects are not always imagined in advance. And changing the model after you have collected the data is among the most malicious thing a frequentist statistician can do.\n",
        "\n",
        "In this way setting bounds by default on the bin widths and locations, kind of ban from looking too close to fine effects on recorded data. But the bins have to be set prior to data collection! So when dealing with large data sets, binning data is good in 2 ways: computation efficiency and avoid revealing your model is indeed wrong, even if it is sufficient and robust to estimate your theory parameters... !!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWFwJLl90d9T"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1_Op--iIAbMPPexLSiYdC2kB_f97QEqV9\" width=800px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLkYDpnLWMsf"
      },
      "source": [
        "#### Plotting histograms as dots with error bars\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gmention-at-cea/ST4_PP_2020/master/Error_bars.png\" width=\"800px\">\n",
        "<figcaption><b>Fig. 2</b> | Drawing count data with a dot and an error bar. Data count follows as Poisson distribution. Here $k$ counts are observed. The estimated variance is thus $k$ and so the standard deviation is $\\sqrt{k}$. The dot is thus displayed at $y=k$ for this bin content and the error bars extend up to $y_{\\rm up} = k+\\sqrt{k}$ and down to $y = k-\\sqrt{k}$ by convention.</figcaption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgHp-pIk9ZCb"
      },
      "source": [
        "## Counting (or frequency) histogram and density histogram\n",
        "\n",
        "Choice of the histogram normalization: pay care to this point. Counting (or frequency) and density."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw4NFfvXs1gY"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/gmention-at-cea/ST4_PP_2020/master/Histograms.png\" width=\"800px\">\n",
        "<figcaption><b>Fig. 2</b> | Two main ways to display histograms. The first one is the basic counting of the number of occurences of $x_1,\\ldots,x_n$ which fall inside the bin range. We call it $N$. The other way is related to the probability density function. The counting $N$ is thus normalised with respect to the bin width $\\Delta{x}$ and the total number of events, $n$, in the data sample $(x_1,\\ldots,x_n)$</figcaption>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_wuUcDgtwAM"
      },
      "source": [
        "# Plot histograms with both normalization\n",
        "# 1. Frequency distribution\n",
        "# 2. Density distribution\n",
        "\n",
        "n_trial = 10\n",
        "success_proba = 0.8\n",
        "B = stats.binom(n_trial, success_proba)\n",
        "n_experiments = 100\n",
        "\n",
        "Brvs = B.rvs(n_experiments)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,3))\n",
        "# Plotting histograms and playing with arguments\n",
        "# using numpy.arange\n",
        "ax1.hist(Brvs, range=(3,12), bins=9, density=False, color='tab:blue',\\\n",
        "         align='left', rwidth=.5) # density = True|default=False, \n",
        "ax1.set_title('Frequency histogram', fontsize=8)\n",
        "\n",
        "# Changing normalization to density\n",
        "ax2.hist(Brvs, range=(3,12), bins=9, density=True, color='tab:red',\\\n",
        "         align='left', rwidth=.5) # density = True|default=False, \n",
        "ax2.set_title('Density histogram', fontsize=8)\n",
        "\n",
        "fig.suptitle('Histograms: plotting frequency or density', fontsize=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKffXGZ6WiAZ"
      },
      "source": [
        "We end up here just with a global normalization effect. Not a big deal. Just take care in handling the normalization and it's done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YatgBpVo9LeR"
      },
      "source": [
        "## Bin width - irregular binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysc0zcmbDYnu"
      },
      "source": [
        "X = stats.norm\n",
        "\n",
        "r = X.rvs(size = 100000)\n",
        "nbins = 20\n",
        "p = np.linspace(0+1e-5,1-1e-5,nbins+1)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
        "ax1.hist(r, bins=X.ppf(p), edgecolor='k', density=False, alpha=1, \\\n",
        "         label='Quantile bins')\n",
        "xmin, xmax = X.ppf([min(p), max(p)])\n",
        "bins_eq = np.linspace(xmin, xmax, nbins+1)\n",
        "ax1.hist(r, bins=bins_eq, edgecolor='none', density=False, alpha=.2, \\\n",
        "         label='Regular bins')\n",
        "ax1.set_title('Frequency histogram', fontsize=8)\n",
        "ax1.legend(frameon=False)\n",
        "\n",
        "ax2.hist(r, bins=X.ppf(p), edgecolor='k', density=True, alpha=1)\n",
        "xmin, xmax = X.ppf([min(p), max(p)])\n",
        "bins_eq = np.linspace(xmin, xmax, nbins+1)\n",
        "ax2.hist(r, bins=bins_eq, edgecolor='k', density=True, alpha=.2)\n",
        "x = np.linspace(xmin,xmax,1000)\n",
        "ax2.plot(x, X.pdf(x), lw=3, ls='--', color='k')\n",
        "ax2.set_title('Density histogram', fontsize=8)\n",
        "\n",
        "fig.suptitle('Different binning definitons: impact of uneven binning on'+ \\\n",
        "              'type of histogram', fontsize=8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGY-jJUfVf3n"
      },
      "source": [
        "Using the quantile (inverse cumulative distribution) to define the bin widths, our collected data sample should look flat with a frequency binning. This means that every bin is expected to have the same statistical fluctuations. It could reveal convenient for graphical check up and for fitting (homeskedastic, i.e. same variance). And we can recover the probability density shape by drawing the density histogram. Both definitions are very convenient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiL6fhgGqsz9"
      },
      "source": [
        "**EXERCISE**:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O48Caxum-Lq_"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "# Define a normal random variable with\n",
        "mu = 1\n",
        "sigma = .5\n",
        "X = stats.norm(loc=mu, scale=sigma)\n",
        "# mean, var, skew, kurt = X.stats(moments='mvsk')\n",
        "x = np.linspace(X.ppf(1e-5),X.ppf(1-1e-5), 100)\n",
        "ax.plot(x, X.pdf(x),'k-', lw=3, alpha=1, label='norm pdf')\n",
        "r = X.rvs(size=1000)\n",
        "ax.hist(r, bins=20, density=True, histtype='stepfilled', alpha=0.5, \\\n",
        "        label='sample histo')\n",
        "ax.legend(loc='best', frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TLNrDnq-KQ6"
      },
      "source": [
        "\n",
        "> **Q1**: From the PDF, what is the probability that X>0?  \n",
        "> **Q2**: From the PDF, what is the 0.92 quantile of X?  \n",
        "> **Q3**: From the PDF and the sample of size 1000, Draw the sample quantile as a function of the theoretical quantile. [QQplot]  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qtHWoDnc8u"
      },
      "source": [
        "# Q1_ans\n",
        "print(1-X.cdf(0))\n",
        "# Q2_ans\n",
        "print(X.ppf(0.92))\n",
        "# Q3_ans\n",
        "r = X.rvs(size = 200)\n",
        "n = len(r)\n",
        "rs = np.sort(r)\n",
        "Qth = np.zeros(len(r))\n",
        "Qobs = np.zeros(len(r))\n",
        "for i, x in enumerate(r):\n",
        "  Qth[i] = X.ppf((i+1)/(n+1))\n",
        "  Qobs[i] = rs[i]\n",
        "\n",
        "def QQvar(X,p,n):\n",
        "  # Asymptotic variance of the quantiles: normal distribution with variance\n",
        "  return p*(1-p)/(n*(X.pdf(X.ppf(p)))**2)\n",
        "\n",
        "# print('QQvar = ', QQvar(X,[.2 .3],10))\n",
        "std = np.asarray([ np.sqrt(QQvar(X,(i+1)/(n+1),len(r))) for i, _ in enumerate(r)])\n",
        "\n",
        "plt.plot(Qth,Qobs, linewidth=3, label=\"sample quantiles\")\n",
        "plt.plot(Qth,Qth, 'k', linestyle='-', linewidth=1, label=\"expected relation\")\n",
        "plt.plot(Qth,Qth+2*std, 'k', linestyle=':', linewidth=1, label=\"2σ CI\")\n",
        "plt.plot(Qth,Qth-2*std, 'k', linestyle=':', linewidth=1)\n",
        "plt.xlabel('Theoretical distribution quantiles')\n",
        "plt.ylabel('sample distribution quantiles')\n",
        "plt.legend( frameon=False)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDri-7x-5lt"
      },
      "source": [
        "A QQplot is a convenient way to check a sample is following a given theoretical prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wico17fS2Pg6"
      },
      "source": [
        "\n",
        "## Averages and CLT, the Central Limit Theorem\n",
        "\n",
        "**Central Limit Theorem in short**: For any distribution with finite variance, adding up a large amount or independent random variables $X_i$ with the same distribution (with finite variance) lead to a normal distribution of the sum of the variables $\\sum_{i=1}^N X_i$. This is the central limit theorem (CLT).\n",
        "\n",
        "We demonstrate below two cases: the coin flip of the dice roll. You can experiment choosing in the menu below and run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "x9FtKvafFZIm"
      },
      "source": [
        "experiment = \"coin\" #@param [\"coin\", \"dice\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-g-RhdyB-AZ"
      },
      "source": [
        "numIterations = np.asarray([1, 2, 5, 10, 50, 100]); #number of i.i.d RVs\n",
        "\n",
        "maxNumForExperiment = {'dice': 6, 'coin': 2} #max numbers represented on dice or coins\n",
        "nSamp=10000\n",
        "\n",
        "k = maxNumForExperiment[experiment]\n",
        "\n",
        "fig, fig_axes = plt.subplots(ncols=3, nrows=2, constrained_layout=True)\n",
        "\n",
        "for i,N in enumerate(numIterations):\n",
        "    y = np.random.randint(low=1, high=k+1, size=(N,nSamp)).sum(axis=0)\n",
        "    row = i//3\n",
        "    col=i%3\n",
        "    bins=np.arange(start=min(y), stop=max(y)+2, step=1)\n",
        "    fig_axes[row, col].hist(y, bins=bins, density=True, align='left')\n",
        "    plural = lambda n: 's' if n>1 else ''\n",
        "    fig_axes[row, col].set_title('N={} {}'.format(N,experiment+plural(N)), fontsize=10)\n",
        "    x = np.linspace(min(y)-2, max(y)+2, 100)\n",
        "    fig_axes[row, col].plot(x, stats.norm.pdf(x, loc=np.mean(y),\\\n",
        "                                              scale=np.std(y)),\\\n",
        "                            color='k', linewidth=1, linestyle='--')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2BseSdOa_KH"
      },
      "source": [
        "An important consequence is the convergence of the average:\n",
        "* as $1/\\sqrt{n}$ (random dispersions of the mean decrease with $n$ as $1/\\sqrt{n}$) if each component of the sum has a finite variance;\n",
        "* and to a normal distribution (CLT, see above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qp_4MoEHimr"
      },
      "source": [
        "y = np.random.randint(low=1, high=7, size=(200,1)).cumsum(axis=0)\n",
        "ymean = y[-1]/len(y)\n",
        "\n",
        "plt.plot([S/(i+1) for i, S in enumerate(y)])\n",
        "plt.plot(ymean + .5/np.sqrt(np.arange(1,len(y)+1)),\\\n",
        "         color='k', linestyle='--', linewidth=1)\n",
        "plt.plot(ymean - .5/np.sqrt(np.arange(1,len(y)+1)),\\\n",
        "         color='k', linestyle='--', linewidth=1)\n",
        "plt.title(r'Convergence of sample average $\\propto\\frac{1}{\\sqrt{n}}$ ', fontsize=12)\n",
        "plt.xlabel('n')\n",
        "plt.ylabel(r'$\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$')\n",
        "plt.show()\n",
        "# for i, S in enumerate(y):\n",
        "#   print('i= %d, S= %.2f' % (i,S/(i+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT8YFEwVH7X5"
      },
      "source": [
        "# Covariance - correlations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Y_x4NmTJmg"
      },
      "source": [
        "\n",
        "## Population and sample covariance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGl3DK49Fuz7"
      },
      "source": [
        "# Population covariance\n",
        "V = np.matrix([[2, 1, 1], [1, 2, 1], [1, 1, 1]])\n",
        "# multivariate normal distribution\n",
        "X = stats.multivariate_normal(cov=V)\n",
        "# Sample of size N\n",
        "N = 10\n",
        "Xrvs = X.rvs(size=N)\n",
        "\n",
        "# Compute the sample covariance matrix\n",
        "# np.cov(Xrvs) # wrong, be careful on dim of X.rvs...\n",
        "np.cov(Xrvs.T)\n",
        "# Then increase size of X.rvs.\n",
        "\n",
        "# Decorrelate the random variables and normalize their variance to 1\n",
        "D, P = np.linalg.eig(V)\n",
        "# P@np.diag(D)@P.T\n",
        "Y = np.diag(1/np.sqrt(D))@P.T@Xrvs.T\n",
        "# Also possible to use the Cholesky decomposition...\n",
        "\n",
        "# Check the decorrelation/normalization transformation is working\n",
        "np.round(np.cov(Y),2)\n",
        "# Try to increase sample size N if not enough..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb4moCesO9w7"
      },
      "source": [
        "Let $\\varepsilon_i$ be independent random variables with mean $0$ and variance $1$.  \n",
        "We define 3 new random variables $X_i$:   \n",
        "> (a) $X_1 = \\varepsilon_1 + \\varepsilon_3$  \n",
        "> (b) $X_2 = \\varepsilon_2 + \\varepsilon_3$  \n",
        "> (c) $X_3 = \\varepsilon_3$  \n",
        "\n",
        "**Q1**: What is the covariance between any $X_i$ and any $X_j$? Write this as a 3x3 matrix.\n",
        "\n",
        "**Q2**: What is the associated correlation matrix? Why is $X_1$ correlated with $X_2$?\n",
        "\n",
        "**Q3**: If $X_3$ is fixed at some given observed value, $X_3=x_3$, what is the correlation between $X_1$ and $X_2$? \n",
        "\n",
        "**Q4**: We call this the partial correlation of $X_1$ and $X_2$ given $X_3$. We note it ${\\rm Cor}[X_1,X_2|X_3]$. What is ${\\rm Cor}[X_1,X_3|X_2]$?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzNa2RM9PB74"
      },
      "source": [
        "from scipy.linalg import inv\n",
        "# Covariance matrix\n",
        "V = np.matrix([[2, 1, 1], [1, 2, 1], [1, 1, 1]])\n",
        "\n",
        "# Computing correlation matrix C from covariance matrix V\n",
        "C = np.asarray([\\\n",
        "                 V[i,j]/np.sqrt(V[i,i]*V[j,j]) \\\n",
        "                 for i in range(V.shape[0]) \\\n",
        "                 for j in range(V.shape[1]) \\\n",
        "                 ]).reshape(V.shape)\n",
        "# Antoher way to compute the correlation matrix\n",
        "# C2 = V/np.sqrt(np.outer(np.diag(V), np.diag(V)))\n",
        "# np.allclose(C2-C,0) # check all coeffs, if True, both ways are equivalent.\n",
        "print('C =', C)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w00amerWPVUg"
      },
      "source": [
        "# Inverse the correlation matrix\n",
        "Cinv = np.round(inv(C))\n",
        "print('Cinv = ', Cinv)\n",
        "\n",
        "# Define Variance Inflation Factors\n",
        "VIF = np.diag(Cinv)\n",
        "print('VIF = ', VIF)\n",
        "\n",
        "c = np.sqrt(VIF)\n",
        "outer_c = np.outer(c, c)\n",
        "# Define partial correlation matrix\n",
        "Cp = 2*np.eye(3)-Cinv/outer_c\n",
        "Cp[Cp == 0] = 0\n",
        "print('Cp = ', Cp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycVb-0KkIAAY"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable ($x$), and the other is considered to be a dependent variable ($y$).  Be careful, linear means linear in the parameter(s) not necessarily in $x$.\n",
        "\n",
        "For example, when we would like to calibrate an instrument, we give some input light or energy or charge deposition to a detector, a sensor and we record some output from the device: a voltage, a current, a count (from a digital counter). The calibration step is to get the trend from the input variables we know, $x$, to the output variables we get experimentally, $y$.\n",
        "\n",
        "Let's do it with some example below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0pNTjJDK9ae"
      },
      "source": [
        "\n",
        "\n",
        "## Ordinary least squares (OLS)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1oQiVDiDBSUAgJLmFzqyRkdJZkESC70ke\" width=800px>\n",
        "\n",
        "The model is thus defined as:\n",
        "$$f(x) = a + b\\times x$$\n",
        "\n",
        "$a$ is called the intercept (value of $y$ at $x=0$). $b$ is called the slope (increment in $y$ by 1 unit of $x$). We assume there is some error in the measurement process, $e$. We assume the instrument response is truly linear and we call $a^\\star$ and $b^\\star$ the true values of $a$ and $b$ above. We collect data samples $(y_1,\\ldots,y_n)$ for different given values $x_1,\\ldots,x_n$ with:\n",
        "$$y_i = a^\\star + b^\\star x_i + e_i$$\n",
        "\n",
        "We do not know $a^\\star$, $b^\\star$ and $e_i$ and we would like to determine their values from $x_i$ and $y_i$. The problem is that we do not know $e_i$ neither and their values change from one measurement to the other.\n",
        "\n",
        "With 2 points we can determine $a$ and $b$. We neglect variations in the $y_i$ caused by the $e_i$. $b = (y_2-y_1)/(x_2-x_1)$, and $a=(y_1+y_2)/2+b\\times(x_1+x_2)/2$. $b$ is sentistive to the spread in $x$ and $y$ while $a$ is sensitive to the average in $x$ and $y$.\n",
        "\n",
        "With many points we can reduce the uncertainty on the determination of $a$ and $b$. Still no real hypothesis on $e_i$. This is the scope of the OLS regression. How to fit?\n",
        "\n",
        "Least squares minimize the distance w.r.t $y$'s values.\n",
        "\n",
        "$$\\sum_{i=1}^n \\left(y_i - (a+b\\times x_i)\\right)^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHLePEUPMdN4"
      },
      "source": [
        "# Fitting\n",
        "\n",
        "Fitting is adjusting the parameters of the model to best describe your data. This operations come down to minimize some \"distance\" between your model and the data.\n",
        "\n",
        "There are plenty of tools (Python packages) to fit data. None of them is universal. They cover different aspects.\n",
        "* [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html): Use non-linear least squares to fit a function, f, to data. \n",
        "* [least squares](http://wwwens.aero.jussieu.fr/lefrere/master/SPE/docs-python/scipy-doc/generated/scipy.optimize.curve_fit.html): Solve a nonlinear least-squares problem with bounds on the variables. \n",
        "* [statsmodels](https://www.statsmodels.org/): estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. \n",
        "* [iMinuit](https://iminuit.readthedocs.io/en/stable/): general robust function minimisation method, but is most commonly used for likelihood fits of models to data, and to get model parameter error estimates from likelihood profile analysis. \n",
        "* [probfit](https://probfit.readthedocs.io/en/latest/): set of functions that helps you construct a complex fit. It’s intended to be used with iminuit. Binned/Unbinned Likelihood estimator, $\\chi^2$ regression, Simultaneous fit estimator... \n",
        "* [myFitter](https://myfitter.hepforge.org): A Python framework for global fits. It features computation of maximum likelihood estimates of model parameters, handles non-linear constraints on the parameter space, computes profile likelihoods,\n",
        "and helps visualisation of one and two-dimensional confidence regions. \n",
        "* [emcee](https://emcee.readthedocs.io/en/stable/) for Markov Chain Monte Carlo with a Bayesian fitting framework.\n",
        "\n",
        "All these tools tend to be rather generic and can be sufficient most of the time. However you could always encounter a problem wich is not tackled by one of this package.\n",
        "\n",
        "For the purpose of this session, we stay however with the basic tools such as `scipy.optimize.minimize` and `scipy.optimize.curve_fit`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW4PidM4MsS_"
      },
      "source": [
        "You have to know how to define a function to be minimized, providing initial values of the parameters, provide bounds on the parameters. There are different methods used in the minimization engine. You could play later on with them. They have different performances (memory, convergence speed, robustness etc.). For what we have to do, don't bother with them.\n",
        "\n",
        "The typical call to minimize is such that:\n",
        "\n",
        "`solution = minimize(my_function, my_init_parameter_guesses, bounds=par_bounds)`\n",
        "\n",
        "`solution` is then a dictionary with the output of the minimization. \n",
        "\n",
        "The parameters at which the minimum occur are stored in `solution.x`. And you can access the to the hessian matrix. The hessian matrix is useful to estimate the covariance among the fitted parameters at the best fit point.\n",
        "\n",
        "To use `curve_fit`, the typical call is:\n",
        "\n",
        "`pars, cov_pars = curve_fit(f, x, y)`\n",
        "\n",
        "`f` is the model function with the parameters to be minimized, `x` and `y` the data points. It provides directly with the best fit parameters in `pars` and the covariance matrix of the fitted parameters in `cov_pars`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpeJkIwMq9M"
      },
      "source": [
        "\n",
        "## Fitting with least squares\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkI3dfkQnOF"
      },
      "source": [
        "from scipy.optimize import curve_fit\n",
        "y = np.array([1.01, 1.09, 1.21])\n",
        "x = np.array([1, 1.08, 1.16])\n",
        "\n",
        "def f(x, a, b):\n",
        "    return a + b*x\n",
        "\n",
        "pars, cov_pars = curve_fit(f, x, y)\n",
        "\n",
        "plt.plot(x, f(x, *pars), label=\"OLS fit\")\n",
        "\n",
        "plt.scatter(x, y, facecolor='k', label=\"data\")\n",
        "\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGSA-nsdZRro"
      },
      "source": [
        "residuals = f(x, *pars) - y\n",
        "residuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hRG2Ua2ZQTb"
      },
      "source": [
        "What are the uncertainties on determined $a$ and $b$?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hico7I17ZX1F"
      },
      "source": [
        "pars_uncertainties = np.sqrt(np.diag(cov_pars))\n",
        "pars_uncertainties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2C6imwIZW34"
      },
      "source": [
        "## Fitting with weigths\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1GgIUmDVvGIWUFxwlStZqwIpqpzzU1J-w\" width=800px>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfNZ6LCXaaX2"
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([0, 0.75, 2])\n",
        "sigma = np.asarray([.1, .25, .12])\n",
        "# Be careful on this call\n",
        "# Here sigma is 1-D, so curve_fit expect standard deviation sigma\n",
        "pars, pcov = curve_fit(f, x, y, sigma=sigma)\n",
        "# Or if sigma is 2-D,  curve_fit expect a covariance matrix\n",
        "# pars, pcov = curve_fit(f, x, y, sigma=np.diag(sigma**2))\n",
        "\n",
        "plt.plot(x, f(x, *pars), label=\"WLS fit\")\n",
        "\n",
        "plt.errorbar(x, y, yerr=sigma, \\\n",
        "             fmt=\"ok\", lw=1, capsize=3, markersize=5, label='data')\n",
        "\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c71FJPl1ZlMd"
      },
      "source": [
        "## Fitting with correlations\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1BDn-TxymvVMDNL25I7PNvkLCuDcO14wT\" width=800px>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R6p2R2ch4bX"
      },
      "source": [
        "Let's start with some fit with a covariance matrix taking into account the correlation between the $y_i$ values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRW1XurZbW4k"
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([0, 0.75, 2])\n",
        "ycov = np.array([[0.101, 0.1  , 0.1  ],\n",
        "                 [0.1  , 0.101, 0.1  ],\n",
        "                 [0.1  , 0.1  , 0.101]])\n",
        "\n",
        "pars, pcov = curve_fit(f, x, y, sigma=ycov)\n",
        "\n",
        "plt.plot(x, f(x, *pars), label=\"GLS fit\")\n",
        "\n",
        "plt.errorbar(x, y, yerr=np.sqrt(np.diag(ycov)), \\\n",
        "             fmt=\"ok\", lw=1, capsize=3, markersize=5, label='data')\n",
        "\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEx6jneSjp5f"
      },
      "source": [
        "So the fit looks ok. The error bars look big and the line close to the points. This type of fit often reflect a large impact due to correlations among the data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p7bExEJj6lA"
      },
      "source": [
        "Let's continue with another case of covariance structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOzZZQeotZyM"
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([0, 0.75, 2])\n",
        "ycov = np.array([[0.0101, 0.02  , 0.01  ],\n",
        "                 [0.02  , 0.0401, 0.02  ],\n",
        "                 [0.01  , 0.02  , 0.0101]])\n",
        "# v = np.matrix([1, 2, 1])\n",
        "# ycov = .0001*np.eye(3) + .01*v.T@v\n",
        "# ycov\n",
        "pars, pcov = curve_fit(f, x, y, sigma=ycov)\n",
        "\n",
        "plt.plot(x, f(x, *pars), label=\"GLS fit\")\n",
        "\n",
        "plt.errorbar(x, y, yerr=np.sqrt(np.diag(ycov)), \\\n",
        "             fmt=\"ok\", lw=1, capsize=3, markersize=5, label='data')\n",
        "\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PaBIBOij_kW"
      },
      "source": [
        "Here, the things look pretty strange. All the data points are on the bottom of the best fit model. This could be a bug. But when you check, this behaviour seems correct. What did happen? Could this be due to covariance matrix of the data points?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imLdmHV8Zm6G"
      },
      "source": [
        "## Fitting with pulls\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1o_FL6b3HcW3jORPRhw7gPOLt-Jy5L46h\" width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJ3ocTbXnbF"
      },
      "source": [
        "So, both approaches are equivalent to determine $a$ and $b$. However when we use the covariance $\\chi^2$ approach, we lose the track of $\\hat{c}$ in the model.\n",
        "\n",
        "Let's define the 2 equivalent $\\chi^2$ functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5rG2WysdHqy"
      },
      "source": [
        "yerr = 0.01*np.array([1, 1, 1])\n",
        "\n",
        "def chi2_pull(pars, x, y, yerr):\n",
        "  a, b, c = pars\n",
        "  z = np.array([1, 2, 1])\n",
        "  sigma_c = 0.1\n",
        "  # weighted chi square definition\n",
        "  chi2 = np.sum((y-f(x, a, b)+z*c)**2/yerr**2)\n",
        "  pull = np.sum(c**2)/sigma_c**2\n",
        "  return chi2 + pull\n",
        "\n",
        "def chi2_cov(pars, x, y, yerr):\n",
        "  a, b = pars\n",
        "  z = np.matrix([1, 2, 1]).T\n",
        "  sigma_c = 0.1\n",
        "  ycov = np.diag(yerr**2) + sigma_c**2*(z@z.T)\n",
        "  Vinv = np.linalg.inv(ycov)\n",
        "  # Generalized chi square definition\n",
        "  chi2 = np.sum((y-f(x, a, b)).T@Vinv@(y-f(x, a, b)))\n",
        "  return chi2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9lXaxAVzWsG"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "# We minimize the chi square with the pull approach: 3 parameters a, b, c\n",
        "soln_pull = minimize(lambda *args: chi2_pull(*args), [1, 1, 1], args=(x, y, yerr))\n",
        "a_pull, b_pull, c_pull = soln_pull.x\n",
        "\n",
        "# We minimize the chi square with the covariance approach: 2 parameters a, b\n",
        "soln_cov  = minimize(lambda *args: chi2_cov(*args), [1, 1], args=(x, y, yerr))\n",
        "a_cov, b_cov = soln_cov.x\n",
        "\n",
        "plt.errorbar(x, y, yerr=np.sqrt(np.diag(ycov)), \\\n",
        "             fmt=\"ok\", lw=1, capsize=3, markersize=5, label='data')\n",
        "z = np.array([1, 2, 1])\n",
        "plt.plot(x,f(x, a_pull, b_pull)-c_pull*z, label='Fit with pulls')\n",
        "plt.plot(x,f(x, a_cov, b_cov), label='Fit with Covariance')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O10SeIilY70z"
      },
      "source": [
        "You see the fit looks much better with the pull approach than with the covariance approach. Let's check the values of $\\hat{a}$ and $\\hat{b}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcVYKHZhZJHr"
      },
      "source": [
        "print('a_pull = ', a_pull)\n",
        "print('a_cov  = ', a_cov)\n",
        "print('\\n')\n",
        "print('b_pull = ', b_pull)\n",
        "print('b_cov  = ', b_cov)\n",
        "print('\\n')\n",
        "print('c_pull = ', c_pull)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7HrkilylAPt"
      },
      "source": [
        "They are almost the same. The difference is only due to numerical accuracy. So the full model \n",
        "$$f(x,a,b,c) = a + b x_i + c z_i$$\n",
        "is able to track the correlation between the data points and indeed in this setup the data covariance matrix is diagonal ${\\rm diag}(\\sigma_1^2,\\ldots,\\sigma_n^2)$. So the best fit behaves as in the WLS case. This particular behaviour of the GLS happen when you have typically large correlations among the data points you provide to the fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B824YQjtmGn2"
      },
      "source": [
        "Let's sum up the situation even further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TteG10NciV8"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1g0dFP1fUSvce7ypnZTwO_37-6tkeyjzY\" width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J02N7dSDmX0T"
      },
      "source": [
        "In this example, $a$ and $b$ are our parameters of interest, and $c$ plays the role of a nuisance parameter. Even if we do not care a priori about the nuisance parameters it is still usefull for the global fit to the data to explain the observed pattern.\n",
        "\n",
        "Covariance and pull approaches are equivalent (in the linear case, and for normal distributions) for what concern a and b however there is more information in the display of the best fit model to $y$ in the pull approach with $c$ taken at the best fit value. The plot then looks more natural...\n",
        "\n",
        "Covariances and correlations can be tricky. Pay attention. Systematic approach with pull terms is more interpretable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv0o1bSGc9Jx"
      },
      "source": [
        "# Plotting the residuals\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qwgkipRS2hd"
      },
      "source": [
        "plt.plot(x,(y-f(x, a_pull, b_pull)+z*c_pull)/yerr, 'ko', ls='none')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SripjQhFZo4Z"
      },
      "source": [
        "\n",
        "\n",
        "## Fitting with errors in $x$ too\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIVVlLqHHbzg"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1QV0dlaynZqlaotIE7b8WN2XVuqvLIKoi\" width=800px>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YNLRa9LUPs0"
      },
      "source": [
        "# For fitting with emcee\n",
        "# first install emcee which is not available in Colab by default\n",
        "# !pip install emcee\n",
        "# Then copy paste the example tutorial on try to understand it.\n",
        "# https://emcee.readthedocs.io/en/stable/tutorials/line/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06x6azOHUhJX"
      },
      "source": [
        "# Define the double negative log likelihood (similar to chi square, of least squares)\n",
        "def dbl_neg_log_likelihood(theta, x, y, yerr):\n",
        "    a, b = theta\n",
        "    model = a + b * x\n",
        "    sigma2 = yerr**2 + a**2*x**2\n",
        "    return np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
        "\n",
        "def least_squares(theta, x, y, yerr):\n",
        "    a, b = theta\n",
        "    model = a + b * x\n",
        "    sigma2 = yerr**2 + a**2*x**2\n",
        "    return np.sum((y - model) ** 2 / sigma2)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9VFiUPUUnGR"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "x = np.linspace(1,10,9)\n",
        "a_true = 1\n",
        "b_true = 2\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "xerr = 1\n",
        "y = a_true + b_true*x\n",
        "x += xerr*np.random.randn(*x.shape)\n",
        "yerr = 1\n",
        "y += yerr*np.random.randn(*y.shape)\n",
        "\n",
        "nll = lambda *args: dbl_neg_log_likelihood(*args)\n",
        "initial = np.array([a_true, b_true]) + 0.1 * np.random.randn(2)\n",
        "soln_ml = minimize(nll, initial, args=(x, y, yerr))\n",
        "a_ml, b_ml = soln_ml.x\n",
        "print(\"Maximum likelihood estimates:\")\n",
        "print(\"a = {0:.3f}\".format(a_ml))\n",
        "print(\"b = {0:.3f}\".format(b_ml))\n",
        "\n",
        "lsq = lambda *args: least_squares(*args)\n",
        "initial = np.array([a_true, b_true]) + 0.1 * np.random.randn(2)\n",
        "soln_lsq = minimize(lsq, initial, args=(x, y, yerr))\n",
        "a_lsq, b_lsq = soln_lsq.x\n",
        "print(\"Least squares estimates:\")\n",
        "print(\"a = {0:.3f}\".format(a_lsq))\n",
        "print(\"b = {0:.3f}\".format(b_lsq))\n",
        "\n",
        "\n",
        "plt.errorbar(x, y, xerr=xerr, yerr=yerr, fmt=\"ok\", lw=1, capsize=3, markersize=5)\n",
        "plt.plot(x, a_true+b_true*x , \"k\", alpha=0.3, lw=2, label=\"truth\")\n",
        "plt.plot(x, a_lsq+b_lsq*x, \"--b\", lw=3, label=\"LS\")\n",
        "plt.plot(x, a_ml+b_ml*x, \"-r\", lw=1, label=\"ML\")\n",
        "plt.legend(frameon=False)\n",
        "plt.xlim(0, 10)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zicY6aJkRwC_"
      },
      "source": [
        "# Maximum likelihood estimation\n",
        "\n",
        "Here the goal is to fit data with likelihood. And to pay attention to the likelihood normalization. We will be fitting an exponential distribution and an exponential distribution with cuts...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ0Pn-GDSC5E"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Generate a sample of size 100 from an exponential distribution $f_X(x,\\lambda) = \\lambda e^{-\\lambda x}$ with $\\lambda = 1/2$. Find for this, the appropriate function in `scipy.stats` package. Call `X` the variable which contains this data sample.\n",
        "\n",
        "1. Check that the mean and the variance are indeed from what you expect from an exponential distribution. Did you pay attention to the arguments to call this function such as `loc` and `scale`?\n",
        "\n",
        "2. Draw a histogram of this sample `X` (Use `matplotlib.pyplot.hist`).\n",
        "\n",
        "3. Get used to `bins`, `range`, `width`, `density` options. Try them and check the behaviour. Especially look at the y-axis scale.\n",
        "\n",
        "4. Plot superimposed the histogram and the exponential PDF from the function definition. How can you make the PDF and the histogram look more alike for what concerns the y-scale?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0YAV5PYR_8k"
      },
      "source": [
        "lb = .5\n",
        "X = stats.expon(loc=0,scale=1/lb).rvs(size=10000)\n",
        "\n",
        "print('mean = %.2f' % np.mean(X))\n",
        "print('var = %.2f' % np.var(X))\n",
        "\n",
        "\n",
        "x = np.linspace(-2,10,100)\n",
        "\n",
        "plt.plot(x,stats.expon(loc=0,scale=1/lb).pdf(x));\n",
        "plt.plot(x,stats.expon(loc=4,scale=1/lb).pdf(x));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN6H5f4ISQLo"
      },
      "source": [
        "bin_edges = np.linspace(0,10,51)\n",
        "plt.hist(X, bins=bin_edges);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-mUgQpkSfgv"
      },
      "source": [
        "bin_edges = (0, 1, 2, 5, 7, 10)\n",
        "plt.hist(X, bins=bin_edges);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNo4pm1DSlwS"
      },
      "source": [
        "bin_edges = (0, 1, 2, 5, 7, 10)\n",
        "plt.hist(X, bins=bin_edges, density=True);\n",
        "plt.plot(x,stats.expon(loc=0,scale=1/lb).pdf(x));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaA68Xd6Sw_J"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "We want now to fit the data set `X` we defined in the previous Question.\n",
        "You will have to use `minimize` function from `scipy.optimize`. Read the documentation.\n",
        "\n",
        "You have to define:\n",
        "1. the PDF of the exponential PDF ($f_X(x;\\lambda)$)\n",
        "2. the loglikelihood ($-2\\log f_X(x;\\lambda)$) of the dataset with this PDF.\n",
        "\n",
        "Then minimize the loglikelihood and plot the result (the histogram with the data and the best fit PDF i.e. the PDF with parameter value obtained from the minimization process of the loglikelihood).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBjXA5SGSpRS"
      },
      "source": [
        "def exp_pdf(x,lb):\n",
        "  return lb*np.exp(-lb*x)\n",
        "\n",
        "def llexp(lb):\n",
        "  return -2*np.log(exp_pdf(X,lb)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMWIGjeYS8x3"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "soln = minimize(llexp,1,bounds=[(0.000001,None)])\n",
        "\n",
        "soln.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gENbsZDTBmo"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Assume now that `X` is still from the exponential distribution with $\\lambda=1/2$ defined above, but for some experimental reason you only have access to data bewteen 1 and 3.\n",
        "\n",
        "1. Use the first question instructions to generate a sample of `X` values of size 1000. Then select only the cases between 1 and 3. For instance use \n",
        "`X=X[(X>1)&(X<3)]`. \n",
        "2. Fit this new dataset with the likelihood defined in previous question. Do you find the same estimate? Why?\n",
        "3. You would have expected do get the same estimate of $\\lambda$ since this is the same generating process. What biases the estimator?\n",
        "4. Check the normalization of the likelihood on the sample space ($x$ values). The likelihood should always be normalized to 1 on the sample space. Define therefore a new likelihood correctly normalized for $x$ values between 1 and 3. Do you get now a reasonable estimator of $\\lambda$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD3Xz32DS-oH"
      },
      "source": [
        "Y = X[(X>1)&(X<3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtrIGPtmTHtb"
      },
      "source": [
        "plt.hist(Y, bins=50, range=(0,10));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNU3dHLTJv6"
      },
      "source": [
        "def llexp(lb):\n",
        "  return -2*np.log(exp_pdf(Y,lb)).sum()\n",
        "\n",
        "res2 = minimize(llexp,1,bounds=[(0.000001,None)])\n",
        "res2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unk_WA9cTMpz"
      },
      "source": [
        "def exp_pdf(x,lb):\n",
        "  return (x>1)*(x<3)*lb*np.exp(-lb*x)/(np.exp(-1*lb)-np.exp(-3*lb))\n",
        "\n",
        "def llexp(lb):\n",
        "  return -2*np.log(exp_pdf(Y,lb)).sum()\n",
        "\n",
        "soln3 = minimize(llexp,1,bounds=[(0.000001,None)])\n",
        "soln3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5c0KYFxVpqP"
      },
      "source": [
        "lb_val = np.linspace(.3,.8,100)\n",
        "plt.plot(lb_val, [llexp(lb) for lb in lb_val])\n",
        "chi2min = llexp(soln3.x)\n",
        "sigma_lb = np.sqrt(2*soln3.hess_inv.matmat(np.eye(1)))\n",
        "sigma_lb = np.sqrt(2*soln3.hess_inv.todense())\n",
        "plt.plot(lb_val, chi2min + [(lb-soln3.x)**2 for lb in lb_val]/sigma_lb**2, \\\n",
        "         linestyle = '--', linewidth=3)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlCIr0d8Oonr"
      },
      "source": [
        "Always pay attention to normalize your likelihood to the range of the data sample if it is restricted."
      ]
    }
  ]
}